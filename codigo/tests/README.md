# Suite de Testing - Proyecto Libro Interactivo

## ğŸ“‹ DescripciÃ³n

Suite completa de testing para el proyecto, con enfoque especial en el caso de prueba **CP01_01 â€” Visualizar capÃ­tulo publicado (Ã©xito)**.

## ğŸ¯ Caso de Prueba Principal: CP01_01

### InformaciÃ³n del Caso de Prueba

- **ID**: CP01_01
- **Caso de Uso**: CU_01 Visualizar contenido
- **DescripciÃ³n**: Accede a un capÃ­tulo existente en estado publicado
- **Ãrea Funcional**: Contenidos
- **Funcionalidad**: Lectura de capÃ­tulo

### Datos de Entrada
- Abrir `/capitulos/{id}` con ID vÃ¡lido y publicado

### Resultado Esperado
- âœ… Se muestran tÃ­tulo, nÃºmero e introducciÃ³n (y resto de campos)
- âœ… Status code 200 OK
- âœ… Se registra la visualizaciÃ³n si aplica
- âœ… Respuesta con estructura correcta segÃºn schema

### Ambiente de Pruebas
- Base de datos con al menos un capÃ­tulo en estado `PUBLICADO`

## ğŸš€ EjecuciÃ³n RÃ¡pida

```bash
# Ejecutar tests de CP01_01 con coverage
cd codigo
./ejecutar_tests.sh

# O directamente con pytest
pytest tests/test_cp01_01_visualizar_capitulo.py -v
```

## ğŸ“Š Opciones de EjecuciÃ³n

### 1. Solo CP01_01 (por defecto)
```bash
./ejecutar_tests.sh
# o
./ejecutar_tests.sh cp01
```

### 2. Todos los tests
```bash
./ejecutar_tests.sh all
```

### 3. Solo tests unitarios
```bash
./ejecutar_tests.sh unit
```

### 4. Solo tests de integraciÃ³n
```bash
./ejecutar_tests.sh integration
```

### 5. EjecuciÃ³n rÃ¡pida (sin coverage)
```bash
./ejecutar_tests.sh quick
```

### 6. Tests en paralelo (mÃ¡s rÃ¡pido)
```bash
./ejecutar_tests.sh parallel
```

## ğŸ“ Estructura de Tests

```
tests/
â”œâ”€â”€ __init__.py                           # InicializaciÃ³n del paquete
â”œâ”€â”€ conftest.py                           # Fixtures compartidos
â”œâ”€â”€ test_cp01_01_visualizar_capitulo.py  # Tests del caso CP01_01
â””â”€â”€ test_models.py                        # Tests unitarios de modelos
```

### Archivos Principales

#### `conftest.py`
Contiene fixtures reutilizables:
- `test_db_engine`: Motor de BD SQLite en memoria
- `test_db_session`: SesiÃ³n de BD para cada test
- `client`: Cliente de prueba de FastAPI
- `capitulo_publicado`: Fixture con capÃ­tulo en estado PUBLICADO
- `capitulo_borrador`: Fixture con capÃ­tulo en BORRADOR
- `multiples_capitulos`: Fixture con varios capÃ­tulos en diferentes estados

#### `test_cp01_01_visualizar_capitulo.py`
Clases de test:
1. **TestCP01_01_VisualizarCapituloPublicado**: Tests principales del caso de uso
2. **TestCP01_01_CasosNegativos**: Tests de casos de error
3. **TestCP01_01_Integracion**: Tests de integraciÃ³n
4. **TestCP01_01_Performance**: Tests de rendimiento
5. **TestCP01_01_Regresion**: Tests de regresiÃ³n

## ğŸ“ˆ Coverage (Cobertura)

### Visualizar Coverage
```bash
# Ejecutar tests con coverage
./ejecutar_tests.sh

# Abrir reporte HTML
xdg-open htmlcov/index.html
```

### Reportes Generados

DespuÃ©s de ejecutar los tests, se generan:

1. **HTML Coverage Report**: `htmlcov/index.html`
   - Reporte visual interactivo
   - Muestra lÃ­neas cubiertas y no cubiertas
   - Porcentaje de cobertura por archivo

2. **HTML Test Report**: `reports/cp01_01_report.html`
   - Reporte de resultados de tests
   - InformaciÃ³n detallada de cada test
   - Screenshots si aplica

3. **JSON Coverage**: `coverage.json`
   - Datos de coverage en formato JSON
   - Para integraciÃ³n con CI/CD

4. **Terminal Report**
   - Muestra en consola las lÃ­neas no cubiertas
   - Resumen por archivo

### Objetivos de Coverage

- âœ… **Excelente**: >= 80%
- âš ï¸ **Bueno**: >= 60%
- âŒ **Bajo**: < 60%

## ğŸ§ª Tipos de Tests

### Tests Unitarios
Prueban funcionalidad individual:
- Modelos de base de datos
- Funciones y mÃ©todos aislados
- Validaciones de datos

```bash
pytest tests/test_models.py -v
```

### Tests de IntegraciÃ³n
Prueban flujos completos:
- Endpoints de API
- InteracciÃ³n con base de datos
- Flujos de usuario

```bash
pytest tests/test_cp01_01_visualizar_capitulo.py::TestCP01_01_Integracion -v
```

### Tests de Performance
Verifican rendimiento:
- Tiempo de respuesta
- MÃºltiples requests simultÃ¡neas

```bash
pytest tests/test_cp01_01_visualizar_capitulo.py::TestCP01_01_Performance -v
```

### Tests de RegresiÃ³n
Aseguran que funcionalidades previas sigan funcionando:
- Compatibilidad con versiones anteriores
- Estructura de datos consistente

```bash
pytest tests/test_cp01_01_visualizar_capitulo.py::TestCP01_01_Regresion -v
```

## ğŸ”§ ConfiguraciÃ³n

### pytest.ini
ConfiguraciÃ³n de pytest con:
- Paths de tests
- Opciones por defecto
- Markers personalizados
- ConfiguraciÃ³n de coverage

### Markers Personalizados
```python
@pytest.mark.unit        # Test unitario
@pytest.mark.integration # Test de integraciÃ³n
@pytest.mark.cp01_01     # EspecÃ­fico de CP01_01
@pytest.mark.performance # Test de rendimiento
@pytest.mark.regression  # Test de regresiÃ³n
@pytest.mark.slow        # Test lento
```

Usar markers:
```bash
pytest -m unit           # Solo tests unitarios
pytest -m cp01_01        # Solo tests de CP01_01
pytest -m "not slow"     # Excluir tests lentos
```

## ğŸ“¦ Dependencias de Testing

Las siguientes dependencias se instalan automÃ¡ticamente:

```txt
pytest>=7.4.0                # Framework de testing
pytest-cov>=4.1.0           # Coverage
pytest-html>=3.2.0          # Reportes HTML
pytest-xdist>=3.3.0         # Tests paralelos
fastapi[all]>=0.104.0       # TestClient
httpx>=0.25.0               # Cliente HTTP para tests
```

## ğŸ¨ Estructura de un Test

```python
def test_visualizar_capitulo_publicado_exitoso(self, client, capitulo_publicado):
    """
    Test Principal CP01_01: Visualizar capÃ­tulo publicado con Ã©xito.
    
    GIVEN: Existe un capÃ­tulo en estado PUBLICADO
    WHEN: Se realiza GET a /capitulos/{id}
    THEN: Retorna 200 OK con datos completos
    """
    # Arrange (Preparar)
    capitulo_id = capitulo_publicado.id_capitulo
    
    # Act (Actuar)
    response = client.get(f"/capitulos/{capitulo_id}")
    
    # Assert (Verificar)
    assert response.status_code == 200
    data = response.json()
    assert data["estado"] == "PUBLICADO"
    assert data["titulo"] == capitulo_publicado.titulo
```

## ğŸ› Debugging de Tests

### Ejecutar test especÃ­fico con output detallado
```bash
pytest tests/test_cp01_01_visualizar_capitulo.py::TestCP01_01_VisualizarCapituloPublicado::test_visualizar_capitulo_publicado_exitoso -vvs
```

### Ver variables locales en fallos
```bash
pytest tests/ -l
```

### Detener en el primer fallo
```bash
pytest tests/ -x
```

### Modo interactivo (PDB)
```bash
pytest tests/ --pdb
```

### Ver prints durante tests
```bash
pytest tests/ -s
```

## ğŸ“Š Ejemplo de Output

```
================================================
  EJECUTANDO TESTS CON COVERAGE
  Caso de Prueba: CP01_01
================================================

ğŸ§ª Ejecutando tests de CP01_01...

tests/test_cp01_01_visualizar_capitulo.py::TestCP01_01_VisualizarCapituloPublicado::test_visualizar_capitulo_publicado_exitoso PASSED [ 10%]
tests/test_cp01_01_visualizar_capitulo.py::TestCP01_01_VisualizarCapituloPublicado::test_visualizar_capitulo_publicado_estructura_respuesta PASSED [ 20%]
...

---------- coverage: platform linux, python 3.13.x -----------
Name                              Stmts   Miss  Cover   Missing
---------------------------------------------------------------
api/routers/capitulos.py             45      2    95%   23-24
db/contenido/models.py               52      3    94%   145-147
---------------------------------------------------------------
TOTAL                               156      8    95%

âœ… TESTS COMPLETADOS EXITOSAMENTE

ğŸ“Š Reportes generados:
   - HTML Coverage: htmlcov/index.html
   - HTML Report: reports/cp01_01_report.html

ğŸ“ˆ Resumen de Coverage:
   Cobertura Total: 95.00%
   âœ… Cobertura EXCELENTE (>= 80%)
```

## ğŸ”„ IntegraciÃ³n Continua

Los tests estÃ¡n listos para integrarse con CI/CD:

```yaml
# Ejemplo .github/workflows/tests.yml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run tests
        run: |
          cd codigo
          pip install -r requirements.txt
          ./ejecutar_tests.sh all
```

## ğŸ“ Notas Importantes

1. **Base de Datos de Test**: Los tests usan SQLite en memoria, NO afectan la BD de producciÃ³n
2. **Aislamiento**: Cada test tiene su propia sesiÃ³n de BD
3. **Estado Limpio**: Los fixtures garantizan estado inicial consistente
4. **Fast Feedback**: Tests rÃ¡pidos para desarrollo Ã¡gil

## ğŸ†˜ SoluciÃ³n de Problemas

### Error: "No module named pytest"
```bash
pip install -r requirements.txt
```

### Error: "Database connection failed"
- Los tests usan SQLite en memoria, no necesitan MySQL
- Si el error persiste, verifica que SQLAlchemy estÃ© instalado

### Tests lentos
```bash
# Usar ejecuciÃ³n paralela
./ejecutar_tests.sh parallel
```

### Coverage bajo
1. Identificar lÃ­neas no cubiertas: `xdg-open htmlcov/index.html`
2. Agregar tests para esas lÃ­neas
3. Ejecutar nuevamente: `./ejecutar_tests.sh`

## ğŸ“š Recursos Adicionales

- [Pytest Documentation](https://docs.pytest.org/)
- [FastAPI Testing](https://fastapi.tiangolo.com/tutorial/testing/)
- [Coverage.py](https://coverage.readthedocs.io/)
- [SQLAlchemy Testing](https://docs.sqlalchemy.org/en/14/orm/session_transaction.html#joining-a-session-into-an-external-transaction-such-as-for-test-suites)

## âœ… Checklist de Testing

- [x] Fixtures configurados
- [x] Tests de CP01_01 implementados
- [x] Tests unitarios de modelos
- [x] Tests de integraciÃ³n
- [x] Tests de performance
- [x] Tests de regresiÃ³n
- [x] Coverage configurado
- [x] Script de ejecuciÃ³n
- [x] DocumentaciÃ³n completa
- [ ] Tests de casos de uso adicionales
- [ ] Tests de autenticaciÃ³n
- [ ] Tests de autorizaciÃ³n

---

**Ãšltima actualizaciÃ³n**: 3 de noviembre de 2025
